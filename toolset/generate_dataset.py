#!/usr/bin/env python3
"""
æ•°æ®é›†ç”Ÿæˆå·¥å…·

å°†å•ä¸ª .fvecs æ–‡ä»¶åˆ†å‰²æˆ query å’Œ base ä¸¤éƒ¨åˆ†ï¼Œå¹¶ç”Ÿæˆå¯¹åº”çš„ groundtruth æ–‡ä»¶ã€‚
æ”¯æŒ GPU åŠ é€Ÿå’Œåˆ†æ‰¹è®¡ç®—ï¼Œé€‚ç”¨äºå¤§è§„æ¨¡æ•°æ®é›†ã€‚
"""

import argparse
import os
import numpy as np
import faiss
from tqdm import tqdm
from faiss_benchmark.utils import (
    fvecs_read, fvecs_write, ivecs_write, 
    get_fvecs_info, fvecs_read_range, fvecs_write_streaming
)


def split_dataset(input_file, num_queries, output_prefix, chunk_size=50000):
    """
    å°†è¾“å…¥çš„ .fvecs æ–‡ä»¶åˆ†å‰²æˆ query å’Œ base ä¸¤éƒ¨åˆ†ï¼ˆæµå¼å¤„ç†ï¼Œå†…å­˜å‹å¥½ï¼‰

    Args:
        input_file: è¾“å…¥çš„ .fvecs æ–‡ä»¶è·¯å¾„
        num_queries: query é›†åˆçš„å‘é‡æ•°é‡
        output_prefix: è¾“å‡ºæ–‡ä»¶çš„å‰ç¼€ï¼ˆä¸å«æ‰©å±•åï¼‰
        chunk_size: æ¯æ¬¡å¤„ç†çš„å‘é‡æ•°é‡ï¼ˆé»˜è®¤50000ï¼Œçº¦200MBå†…å­˜ï¼‰

    Returns:
        tuple: (query_vectors, base_vectors) åˆ†åˆ«ä¸º query å’Œ base å‘é‡æ•°ç»„
    """
    print(f"æ­£åœ¨åˆ†ææ•°æ®æ–‡ä»¶: {input_file}")
    
    # è·å–æ–‡ä»¶ä¿¡æ¯ï¼Œä¸åŠ è½½æ•°æ®
    total_vectors, dimension = get_fvecs_info(input_file)
    
    if total_vectors < num_queries:
        raise ValueError(
            f"æ•°æ®é›†æ€»æ•°é‡ ({total_vectors}) å°äºè¯·æ±‚çš„ query æ•°é‡ ({num_queries})"
        )

    num_base = total_vectors - num_queries
    
    print(f"æ•°æ®é›†ä¿¡æ¯: {total_vectors} ä¸ªå‘é‡, {dimension} ç»´")
    print(f"å°†åˆ†å‰²ä¸º: {num_queries} ä¸ª query å‘é‡, {num_base} ä¸ª base å‘é‡")
    
    # è¾“å‡ºæ–‡ä»¶è·¯å¾„
    query_file = f"{output_prefix}_query.fvecs"
    base_file = f"{output_prefix}_base.fvecs"

    # æµå¼å¤„ç† query å‘é‡
    print(f"æ­£åœ¨ä¿å­˜ query å‘é‡åˆ°: {query_file}")
    
    def query_generator():
        remaining = num_queries
        current_idx = 0
        
        while remaining > 0:
            batch_size = min(chunk_size, remaining)
            vectors = fvecs_read_range(input_file, current_idx, batch_size)
            yield vectors
            
            current_idx += batch_size
            remaining -= batch_size
    
    fvecs_write_streaming(query_file, query_generator(), num_queries)
    
    # æµå¼å¤„ç† base å‘é‡
    print(f"æ­£åœ¨ä¿å­˜ base å‘é‡åˆ°: {base_file}")
    
    def base_generator():
        remaining = num_base
        current_idx = num_queries
        
        while remaining > 0:
            batch_size = min(chunk_size, remaining)
            vectors = fvecs_read_range(input_file, current_idx, batch_size)
            yield vectors
            
            current_idx += batch_size
            remaining -= batch_size
    
    fvecs_write_streaming(base_file, base_generator(), num_base)
    
    print("æ•°æ®åˆ†å‰²å®Œæˆ!")
    
    # ä¸ºäº†ä¿æŒå‘åå…¼å®¹ï¼Œè¿”å›å°æ‰¹é‡çš„æ•°æ®ç”¨äºåç»­å¤„ç†
    # åªåŠ è½½å°‘é‡æ•°æ®åˆ°å†…å­˜ï¼Œè€Œä¸æ˜¯å…¨éƒ¨
    sample_size = min(1000, num_queries)
    query_sample = fvecs_read_range(input_file, 0, sample_size)
    
    sample_size = min(1000, num_base)
    base_sample = fvecs_read_range(input_file, num_queries, sample_size)
    
    return query_sample, base_sample


def check_memory_feasibility(num_vectors, dimension, gpu_memory_gb=10):
    """
    æ£€æŸ¥åœ¨ç»™å®š GPU å†…å­˜ä¸‹æ˜¯å¦å¯ä»¥å»ºç«‹ç´¢å¼•
    
    Args:
        num_vectors: å‘é‡æ•°é‡
        dimension: å‘é‡ç»´åº¦
        gpu_memory_gb: GPU å†…å­˜å¤§å° (GB)
        
    Returns:
        dict: åŒ…å«å¯è¡Œæ€§åˆ†æå’Œå»ºè®®çš„å­—å…¸
    """
    bytes_per_float = 4
    
    # è®¡ç®—æ‰€éœ€å†…å­˜
    data_memory_gb = (num_vectors * dimension * bytes_per_float) / (1024**3)
    index_memory_gb = data_memory_gb * 1.1  # 10% å¼€é”€
    
    # è®¡ç®—åˆ†å—æ–¹æ¡ˆ
    available_memory_gb = gpu_memory_gb - 1  # é¢„ç•™ 1GB
    max_vectors_per_chunk = int((available_memory_gb * (1024**3)) / (dimension * bytes_per_float * 1.1))
    num_chunks = (num_vectors + max_vectors_per_chunk - 1) // max_vectors_per_chunk
    
    return {
        'total_memory_required_gb': index_memory_gb,
        'gpu_memory_gb': gpu_memory_gb,
        'feasible': index_memory_gb <= gpu_memory_gb,
        'max_vectors_per_chunk': max_vectors_per_chunk,
        'num_chunks_needed': num_chunks,
        'chunk_memory_gb': (max_vectors_per_chunk * dimension * bytes_per_float * 1.1) / (1024**3)
    }


def calculate_dynamic_batch_size(current_vectors, total_vectors, dimension, gpu_memory_gb, initial_batch_size=50000):
    """
    æ ¹æ®å½“å‰ç´¢å¼•å¤§å°åŠ¨æ€è®¡ç®—æ‰¹å¤„ç†å¤§å°
    
    Args:
        current_vectors: å½“å‰å·²æ·»åŠ çš„å‘é‡æ•°
        total_vectors: æ€»å‘é‡æ•°
        dimension: å‘é‡ç»´åº¦
        gpu_memory_gb: GPU å†…å­˜å¤§å°
        initial_batch_size: åˆå§‹æ‰¹å¤„ç†å¤§å°
        
    Returns:
        int: å»ºè®®çš„æ‰¹å¤„ç†å¤§å°
    """
    bytes_per_float = 4
    
    # è®¡ç®—å½“å‰ç´¢å¼•å ç”¨çš„å†…å­˜
    current_index_memory_gb = (current_vectors * dimension * bytes_per_float * 2.2) / (1024**3)
    
    # é¢„ç•™å†…å­˜ï¼ˆç³»ç»Ÿ + ç¼“å†²åŒºï¼‰
    reserved_memory_gb = 1.5
    
    # è®¡ç®—å¯ç”¨å†…å­˜
    available_memory_gb = gpu_memory_gb - current_index_memory_gb - reserved_memory_gb
    
    if available_memory_gb <= 0:
        # å†…å­˜ä¸è¶³ï¼Œä½¿ç”¨æœ€å°æ‰¹å¤§å°
        return min(1000, initial_batch_size)
    
    # è®¡ç®—æœ€å¤§å®‰å…¨æ‰¹å¤§å°ï¼ˆè€ƒè™‘ 1.5x å¼€é”€ï¼‰
    max_safe_batch_size = int(available_memory_gb * 1024**3 / (dimension * bytes_per_float * 1.5))
    
    # é™åˆ¶åœ¨åˆç†èŒƒå›´å†…
    min_batch_size = 1000
    max_batch_size = min(100000, initial_batch_size * 2)
    
    optimal_batch_size = max(min_batch_size, min(max_safe_batch_size, max_batch_size))
    
    return optimal_batch_size


def create_index(base_file_or_vectors, use_gpu=False, chunk_size=50000, gpu_memory_gb=10, ngpu: int | None = None, gpu_shard: bool = True):
    """
    åˆ›å»ºç´¢å¼•ç”¨äº groundtruth ç”Ÿæˆï¼ˆæ”¯æŒæµå¼åŠ è½½å’Œå†…å­˜æ£€æŸ¥ï¼‰

    Args:
        base_file_or_vectors: base å‘é‡æ–‡ä»¶è·¯å¾„æˆ–å‘é‡æ•°ç»„
        use_gpu: æ˜¯å¦ä½¿ç”¨ GPU
        chunk_size: æµå¼åŠ è½½æ—¶çš„æ‰¹å¤„ç†å¤§å°
        gpu_memory_gb: GPU å†…å­˜å¤§å° (GB)ï¼Œç”¨äºå†…å­˜å¯è¡Œæ€§æ£€æŸ¥

    Returns:
        faiss.Index: åˆ›å»ºçš„ç´¢å¼•
    """
    # åˆ¤æ–­è¾“å…¥ç±»å‹
    if isinstance(base_file_or_vectors, str):
        # æ–‡ä»¶è·¯å¾„ï¼Œä½¿ç”¨æµå¼åŠ è½½
        base_file = base_file_or_vectors
        total_vectors, dimension = get_fvecs_info(base_file)
        
        print(f"ä»æ–‡ä»¶æµå¼åŠ è½½ base å‘é‡: {base_file}")
        print(f"å‘é‡ä¿¡æ¯: {total_vectors} ä¸ªå‘é‡, {dimension} ç»´")
        
        # å¦‚æœä½¿ç”¨ GPUï¼Œæ£€æŸ¥å†…å­˜å¯è¡Œæ€§
        if use_gpu:
            memory_check = check_memory_feasibility(total_vectors, dimension, gpu_memory_gb)
            print(f"\nğŸ” GPU å†…å­˜å¯è¡Œæ€§æ£€æŸ¥:")
            print(f"  æ‰€éœ€å†…å­˜: {memory_check['total_memory_required_gb']:.2f} GB")
            print(f"  å¯ç”¨å†…å­˜: {memory_check['gpu_memory_gb']} GB")
            
            if not memory_check['feasible']:
                print(f"  âŒ å†…å­˜ä¸è¶³ï¼è¶…å‡º {memory_check['total_memory_required_gb'] - memory_check['gpu_memory_gb']:.2f} GB")
                print(f"\nğŸ’¡ å»ºè®®çš„åˆ†å—æ–¹æ¡ˆ:")
                print(f"  æ¯å—æœ€å¤§å‘é‡æ•°: {memory_check['max_vectors_per_chunk']:,}")
                print(f"  éœ€è¦çš„å—æ•°: {memory_check['num_chunks_needed']}")
                print(f"  æ¯å—å†…å­˜å ç”¨: {memory_check['chunk_memory_gb']:.2f} GB")
                print(f"\nâš ï¸  è­¦å‘Š: å½“å‰é…ç½®å¯èƒ½å¯¼è‡´ GPU å†…å­˜æº¢å‡º")
                print(f"     å»ºè®®ä½¿ç”¨ CPU æ¨¡å¼æˆ–å‡å°‘æ•°æ®é‡")
            else:
                print(f"  âœ… å†…å­˜å……è¶³ï¼Œå¯ä»¥ä½¿ç”¨ GPU æ¨¡å¼")
        
        print()
        
        # ç¡®ä¿ dimension æ˜¯æ­£ç¡®çš„æ•´æ•°ç±»å‹
        dimension = int(dimension)
        
        if use_gpu:
            # æ£€æŸ¥ GPU å¯ç”¨æ€§
            n_available = faiss.get_num_gpus()
            if n_available <= 0:
                raise RuntimeError("æœªæ£€æµ‹åˆ°å¯ç”¨çš„ GPU")
            # é€‰æ‹©å¤š GPU æ•°é‡
            if ngpu is None:
                ngpu = n_available
            ngpu = max(1, min(ngpu, n_available))

            print(f"ä½¿ç”¨ {ngpu} ä¸ª GPU åˆ›å»ºç´¢å¼• (ç»´åº¦: {dimension}) | shard={gpu_shard}")
            # åˆ›å»º CPU åŸºç´¢å¼•å¹¶å…‹éš†åˆ°æ‰€æœ‰å¯è§ GPU
            index_cpu = faiss.IndexFlatL2(int(dimension))
            co = faiss.GpuMultipleClonerOptions()
            co.shard = bool(gpu_shard)
            # ä½¿ç”¨æ‰€æœ‰å¯è§ GPUï¼›å¦‚éœ€æŒ‡å®š GPUï¼Œè¯·è®¾ç½® CUDA_VISIBLE_DEVICES ç¯å¢ƒå˜é‡
            index = faiss.index_cpu_to_all_gpus(index_cpu, co)
        else:
            print(f"ä½¿ç”¨ CPU åˆ›å»ºç´¢å¼• (ç»´åº¦: {dimension})")
            index = faiss.IndexFlatL2(int(dimension))

        # æµå¼æ·»åŠ å‘é‡åˆ°ç´¢å¼•
        print(f"æ­£åœ¨æµå¼æ·»åŠ  {total_vectors} ä¸ªå‘é‡åˆ°ç´¢å¼•...")
        if use_gpu:
            # å–æ¶ˆåŠ¨æ€æ‰¹å¤§å°è°ƒæ•´ï¼Œå§‹ç»ˆä½¿ç”¨ç”¨æˆ·æä¾›çš„ batch_size
            print(f"ä½¿ç”¨å›ºå®šæ‰¹å¤„ç†å¤§å°ï¼ˆGPU æ¨¡å¼ï¼‰ï¼š{chunk_size}")
        
        current_idx = 0
        remaining = total_vectors
        batch_count = 0
        
        with tqdm(total=total_vectors, desc="æ·»åŠ å‘é‡") as pbar:
            while remaining > 0:
                # å–æ¶ˆåŠ¨æ€æ‰¹æ¬¡è°ƒæ•´ï¼Œå§‹ç»ˆä½¿ç”¨é…ç½®çš„ chunk_size
                batch_size = min(chunk_size, remaining)
                
                vectors = fvecs_read_range(base_file, current_idx, batch_size)
                
                # æ·»åŠ åˆ°ç´¢å¼•
                index.add(vectors.astype(np.float32))
                
                current_idx += batch_size
                remaining -= batch_size
                batch_count += 1
                pbar.update(batch_size)
                
                # æ¸…ç†å†…å­˜
                del vectors
        
    else:
        # å‘é‡æ•°ç»„ï¼Œä¿æŒåŸæœ‰é€»è¾‘
        base_vectors = base_file_or_vectors
        dimension = int(base_vectors.shape[1])

        if use_gpu:
            # æ£€æŸ¥ GPU å¯ç”¨æ€§
            n_available = faiss.get_num_gpus()
            if n_available <= 0:
                raise RuntimeError("æœªæ£€æµ‹åˆ°å¯ç”¨çš„ GPU")
            if ngpu is None:
                ngpu = n_available
            ngpu = max(1, min(ngpu, n_available))

            print(f"ä½¿ç”¨ {ngpu} ä¸ª GPU åˆ›å»ºç´¢å¼• (ç»´åº¦: {dimension}) | shard={gpu_shard}")
            index_cpu = faiss.IndexFlatL2(int(dimension))
            co = faiss.GpuMultipleClonerOptions()
            co.shard = bool(gpu_shard)
            index = faiss.index_cpu_to_all_gpus(index_cpu, co)
        else:
            print(f"ä½¿ç”¨ CPU åˆ›å»ºç´¢å¼• (ç»´åº¦: {dimension})")
            index = faiss.IndexFlatL2(int(dimension))

        # æ·»åŠ  base å‘é‡åˆ°ç´¢å¼•
        print(f"æ·»åŠ  {len(base_vectors)} ä¸ªå‘é‡åˆ°ç´¢å¼•...")
        index.add(base_vectors.astype(np.float32))

    return index





def estimate_memory_usage(num_base, num_queries, dimension, topk, use_gpu=False):
    """
    ä¼°ç®—å†…å­˜ä½¿ç”¨é‡

    Args:
        num_base: base å‘é‡æ•°é‡
        num_queries: query å‘é‡æ•°é‡
        dimension: å‘é‡ç»´åº¦
        topk: è¿”å›çš„æœ€è¿‘é‚»æ•°é‡
        use_gpu: æ˜¯å¦ä½¿ç”¨ GPU

    Returns:
        dict: å†…å­˜ä½¿ç”¨ä¼°ç®—
    """
    # åŸºç¡€æ•°æ®å†…å­˜ (float32)
    base_memory = num_base * dimension * 4  # bytes
    query_memory = num_queries * dimension * 4  # bytes

    # ç´¢å¼•å†…å­˜ (å¤§çº¦ç­‰äºåŸºç¡€æ•°æ®)
    index_memory = base_memory

    # ç»“æœå†…å­˜
    result_memory = num_queries * topk * 4  # int32

    total_memory = base_memory + query_memory + index_memory + result_memory

    return {
        "base_vectors": base_memory / (1024**3),  # GB
        "query_vectors": query_memory / (1024**3),  # GB
        "index": index_memory / (1024**3),  # GB
        "results": result_memory / (1024**3),  # GB
        "total": total_memory / (1024**3),  # GB
        "device": "GPU" if use_gpu else "CPU",
    }


def suggest_batch_size(num_queries, available_memory_gb=16.0):
    """
    æ ¹æ®å¯ç”¨å†…å­˜å»ºè®®æ‰¹å¤„ç†å¤§å°

    Args:
        num_queries: query å‘é‡æ•°é‡
        available_memory_gb: å¯ç”¨å†…å­˜ (GB)

    Returns:
        int: å»ºè®®çš„æ‰¹å¤„ç†å¤§å°
    """
    # ä¿å®ˆä¼°è®¡ï¼Œå‡è®¾æ¯ä¸ª query éœ€è¦ 1KB å†…å­˜ç”¨äºä¸­é—´è®¡ç®—
    memory_per_query = 0.001  # GB
    max_batch = int(available_memory_gb / memory_per_query)

    # é™åˆ¶åœ¨åˆç†èŒƒå›´å†…
    suggested_batch = min(max_batch, max(100, num_queries // 10))
    return suggested_batch


def main():
    parser = argparse.ArgumentParser(
        description="å°† .fvecs æ–‡ä»¶åˆ†å‰²æˆ query/base å¹¶ç”Ÿæˆ groundtruth (æ”¯æŒ GPU åŠ é€Ÿ)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # CPU æ¨¡å¼ï¼ˆè¾“å…¥ä¸ºæ•°æ®é›†ç›®å½•è·¯å¾„ï¼Œç›®å½•åå³æ•°æ®é›†åï¼‰
  python generate_dataset.py -i data/sift -q 1000 -k 100

  # GPU æ¨¡å¼ï¼Œè‡ªåŠ¨é€‰æ‹©æ‰¹å¤„ç†å¤§å°
  python generate_dataset.py -i data/sift -q 1000 -k 100 --gpu

  # GPU æ¨¡å¼ï¼Œè‡ªå®šä¹‰æ‰¹å¤„ç†å¤§å°
  python generate_dataset.py -i data/sift -q 1000 -k 100 --gpu --batch-size 500

  # GPU æ¨¡å¼ï¼ŒæŒ‡å®š GPU å†…å­˜å¤§å°ï¼ˆç”¨äºå¯è¡Œæ€§æ£€æŸ¥ä¸åˆ†å—å»ºè®®ï¼‰
  python generate_dataset.py -i data/large -q 1000 -k 100 --gpu --gpu-memory 24

  è¿™å°†ä¼šåœ¨æ•°æ®é›†ç›®å½•ä¸‹ç”Ÿæˆ:
  - data/sift/sift_query.fvecs (1000 ä¸ª query å‘é‡)
  - data/sift/sift_base.fvecs (å‰©ä½™çš„ base å‘é‡)
  - data/sift/sift_groundtruth.ivecs (æ¯ä¸ª query çš„å‰ 100 ä¸ªæœ€è¿‘é‚»)

  æ³¨æ„:
  - è¾“å…¥å‚æ•° -i/--input éœ€è¦æä¾›æ•°æ®é›†ç›®å½•è·¯å¾„ï¼Œç›®å½•ä¸‹éœ€åŒ…å«åŒå .fvecs æ–‡ä»¶ï¼Œä¾‹å¦‚ data/sift/sift.fvecsã€‚
  - ä½¿ç”¨ --gpu æ—¶ï¼Œä¼šè¿›è¡Œ GPU å†…å­˜å¯è¡Œæ€§æ£€æŸ¥ï¼›è‹¥ä¸è¶³å°†æç¤ºåˆ†å—å»ºè®®ã€‚
        """,
    )

    parser.add_argument("-i", "--input", required=True, help="è¾“å…¥çš„æ•°æ®é›†ç›®å½•è·¯å¾„")
    parser.add_argument(
        "-q", "--queries", type=int, required=True, help="query é›†åˆçš„å‘é‡æ•°é‡"
    )
    parser.add_argument(
        "-k",
        "--topk",
        type=int,
        default=100,
        help="æ¯ä¸ª query çš„æœ€è¿‘é‚»æ•°é‡ (é»˜è®¤: 100)",
    )
    parser.add_argument(
        "--gpu", action="store_true", help="ä½¿ç”¨ GPU åŠ é€Ÿ groundtruth ç”Ÿæˆ"
    )
    parser.add_argument(
        "--ngpu", type=int, default=0, help="ä½¿ç”¨çš„ GPU æ•°é‡ (é»˜è®¤ 0 è¡¨ç¤ºä½¿ç”¨æ‰€æœ‰å¯ç”¨ GPU)"
    )
    parser.add_argument(
        "--gpu-shard", action="store_true", default=True, help="å¯ç”¨åˆ†ç‰‡æ¨¡å¼ï¼ˆè·¨ GPU åˆ†ç‰‡è€Œéå¤åˆ¶ï¼Œé»˜è®¤å¼€å¯ï¼‰"
    )
    parser.add_argument(
        "--no-gpu-shard", dest="gpu_shard", action="store_false", help="å…³é—­åˆ†ç‰‡æ¨¡å¼ï¼Œæ”¹ä¸ºåœ¨æ¯ä¸ª GPU å¤åˆ¶å®Œæ•´ç´¢å¼•"
    )
    parser.add_argument(
        "--batch-size", type=int, default=0, help="æ‰¹å¤„ç†å¤§å° (0 è¡¨ç¤ºè‡ªåŠ¨é€‰æ‹©)"
    )
    parser.add_argument(
        "--memory-limit",
        type=float,
        default=16.0,
        help="å†…å­˜é™åˆ¶ (GBï¼Œç”¨äºè‡ªåŠ¨é€‰æ‹©æ‰¹å¤„ç†å¤§å°ï¼Œé»˜è®¤: 16.0)",
    )
    parser.add_argument(
        "--gpu-memory",
        type=float,
        default=10.0,
        help="GPU å†…å­˜å¤§å° (GBï¼Œç”¨äºå†…å­˜å¯è¡Œæ€§æ£€æŸ¥ï¼Œé»˜è®¤: 10.0)",
    )

    args = parser.parse_args()
    
    data_base_dir = args.input
    if not os.path.exists(data_base_dir):
        print(f"é”™è¯¯: æ•°æ®ç›®å½• {data_base_dir} ä¸å­˜åœ¨")
        return 1
    data_name = os.path.basename(data_base_dir)
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    input_file = os.path.join(data_base_dir, f"{data_name}.fvecs")
    if not os.path.exists(input_file):
        print(f"é”™è¯¯: è¾“å…¥æ•°æ® {input_file} ä¸å­˜åœ¨")
        return 1

    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ‰©å±•å
    if not input_file.endswith(".fvecs"):
        print(f"é”™è¯¯: è¾“å…¥æ–‡ä»¶å¿…é¡»æ˜¯ .fvecs æ ¼å¼")
        return 1

    try:
        # è·å–æ•°æ®é›†ä¿¡æ¯ï¼ˆä¸åŠ è½½æ•°æ®ï¼‰
        total_vectors, dimension = get_fvecs_info(input_file)
        num_base = total_vectors - args.queries
        
        print(f"\næ•°æ®é›†ä¿¡æ¯:")
        print(f"  æ€»å‘é‡æ•°: {total_vectors}")
        print(f"  ç»´åº¦: {dimension}")
        print(f"  Query å‘é‡æ•°: {args.queries}")
        print(f"  Base å‘é‡æ•°: {num_base}")

        # åˆ†å‰²æ•°æ®é›†ï¼ˆæµå¼å¤„ç†ï¼Œå†…å­˜å‹å¥½ï¼‰
        output_prefix = os.path.join(data_base_dir, data_name)
        query_sample, base_sample = split_dataset(
            input_file, args.queries, output_prefix
        )

        # ä¼°ç®—å†…å­˜ä½¿ç”¨ï¼ˆåŸºäºå®é™…æ•°æ®é‡ï¼‰
        memory_info = estimate_memory_usage(
            num_base,
            args.queries,
            dimension,
            args.topk,
            args.gpu,
        )

        print(f"\nå†…å­˜ä½¿ç”¨ä¼°ç®— ({memory_info['device']}):")
        print(f"  Base å‘é‡: {memory_info['base_vectors']:.2f} GB")
        print(f"  Query å‘é‡: {memory_info['query_vectors']:.2f} GB")
        print(f"  ç´¢å¼•: {memory_info['index']:.2f} GB")
        print(f"  ç»“æœ: {memory_info['results']:.2f} GB")
        print(f"  æ€»è®¡: {memory_info['total']:.2f} GB")

        # ç¡®å®šæ‰¹å¤„ç†å¤§å°
        if args.batch_size == 0:
            batch_size = suggest_batch_size(args.queries, args.memory_limit)
            print(f"\nè‡ªåŠ¨é€‰æ‹©æ‰¹å¤„ç†å¤§å°: {batch_size}")
        else:
            batch_size = args.batch_size
            print(f"\nä½¿ç”¨æŒ‡å®šæ‰¹å¤„ç†å¤§å°: {batch_size}")

        # æ¸…ç†æ ·æœ¬æ•°æ®ï¼Œé‡Šæ”¾å†…å­˜
        del query_sample, base_sample
        
        # åˆ›å»ºç´¢å¼•ï¼ˆä½¿ç”¨æµå¼åŠ è½½ï¼‰
        base_file = f"{output_prefix}_base.fvecs"
        ngpu_val = None if args.ngpu == 0 else int(args.ngpu)
        index = create_index(base_file, args.gpu, gpu_memory_gb=args.gpu_memory, ngpu=ngpu_val, gpu_shard=bool(args.gpu_shard))

        # ç”Ÿæˆ groundtruthï¼ˆä½¿ç”¨æµå¼åŠ è½½ query å‘é‡ï¼‰
        query_file = f"{output_prefix}_query.fvecs"
        groundtruth_file = f"{output_prefix}_groundtruth.ivecs"
        
        print(f"\næ­£åœ¨ç”Ÿæˆ groundtruth...")
        
        # æµå¼å¤„ç† query å‘é‡ç”Ÿæˆ groundtruth
        all_groundtruth = []
        current_idx = 0
        remaining_queries = args.queries
        
        with tqdm(total=args.queries, desc="ç”Ÿæˆ groundtruth") as pbar:
            while remaining_queries > 0:
                current_batch_size = min(batch_size, remaining_queries)
                
                # è¯»å–å½“å‰æ‰¹æ¬¡çš„ query å‘é‡
                query_batch = fvecs_read_range(query_file, current_idx, current_batch_size)
                
                # æœç´¢æœ€è¿‘é‚»
                _, batch_groundtruth = index.search(query_batch.astype(np.float32), args.topk)
                all_groundtruth.append(batch_groundtruth)
                
                current_idx += current_batch_size
                remaining_queries -= current_batch_size
                pbar.update(current_batch_size)
                
                # æ¸…ç†å½“å‰æ‰¹æ¬¡æ•°æ®
                del query_batch
        
        # åˆå¹¶æ‰€æœ‰ groundtruth
        groundtruth = np.vstack(all_groundtruth)
        del all_groundtruth  # æ¸…ç†ä¸­é—´æ•°æ®

        # ä¿å­˜ groundtruth
        print(f"ä¿å­˜ groundtruth åˆ°: {groundtruth_file}")
        ivecs_write(groundtruth_file, groundtruth.astype(np.int32))

        print("\næ•°æ®é›†ç”Ÿæˆå®Œæˆ!")
        print(f"Query å‘é‡æ•°é‡: {args.queries}")
        print(f"Base å‘é‡æ•°é‡: {num_base}")
        print(f"Groundtruth topk: {args.topk}")
        print(f"ä½¿ç”¨è®¾å¤‡: {'GPU' if args.gpu else 'CPU'}")
        print(f"æ‰¹å¤„ç†å¤§å°: {batch_size}")

    except Exception as e:
        print(f"é”™è¯¯: {e}")
        return 1

    return 0


if __name__ == "__main__":
    exit(main())
